{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCcoWGIbM7lB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def perceptron(x, w, b):\n",
        "    yin = x * w + b\n",
        "    ynet = sigmoid(yin)\n",
        "    return ynet\n",
        "\n",
        "def sigmoid(yin):\n",
        "    return 1 / (1 + np.exp(-yin))\n",
        "\n",
        "def grad_w(w, b, x, y, alpha):\n",
        "    yhat = perceptron(x, w, b)\n",
        "    dw = alpha * (y - yhat) * yhat * (1 - yhat) * x\n",
        "    return dw\n",
        "\n",
        "def grad_b(w, b, x, y, alpha):\n",
        "    yhat = perceptron(x, w, b)\n",
        "    db = alpha * (y - yhat) * yhat * (1 - yhat)\n",
        "    return db\n",
        "\n",
        "def adam_gradient_descent(X, Y, w, b, epochs, alpha, beta1, beta2, eps):\n",
        "    wlist, blist, losslist, accuracylist = [w], [b], [], []\n",
        "    mw, mb, vw, vb = 0, 0, 0, 0\n",
        "\n",
        "    for i in range(1, epochs + 1):\n",
        "        dw, db, total_loss = 0, 0, 0\n",
        "        yhat_list=[]\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            yhat = perceptron(x, w, b)\n",
        "            loss = y - yhat\n",
        "            total_loss += loss ** 2\n",
        "            yhat_list.append(yhat)\n",
        "\n",
        "            dw += grad_w(w, b, x, y, alpha)\n",
        "            db += grad_b(w, b, x, y, alpha)\n",
        "\n",
        "        mw = beta1 * mw + (1 - beta1) * dw\n",
        "        mb = beta1 * mb + (1 - beta1) * db\n",
        "\n",
        "        vw = beta2 * vw + (1 - beta2) * (dw ** 2)\n",
        "        vb = beta2 * vb + (1 - beta2) * (db ** 2)\n",
        "\n",
        "        mw_hat = mw / (1 - beta1 ** i)\n",
        "        mb_hat = mb / (1 - beta1 ** i)\n",
        "        vw_hat = vw / (1 - beta2 ** i)\n",
        "        vb_hat = vb / (1 - beta2 ** i)\n",
        "\n",
        "        w += alpha / (np.sqrt(vw_hat) + eps) * mw_hat\n",
        "        b += alpha / (np.sqrt(vb_hat) + eps) * mb_hat\n",
        "\n",
        "        avg_loss = total_loss / len(X)\n",
        "        accuracy = r2_score(Y,yhat_list)\n",
        "\n",
        "        losslist.append(avg_loss)\n",
        "        accuracylist.append(accuracy)\n",
        "        wlist.append(w)\n",
        "        blist.append(b)\n",
        "\n",
        "    return w, b, wlist, blist, losslist, accuracylist\n",
        "\n",
        "#Q\n",
        "X = [0.5, 2.5]\n",
        "Y = [0.2, 0.9]\n",
        "w, b = 0.0, 0.0\n",
        "epochs = 3000\n",
        "alpha = 0.01\n",
        "beta1 = 0.9  # Exponential decay rate for the first moment estimates\n",
        "beta2 = 0.999  # Exponential decay rate for the second moment estimates\n",
        "eps = 1e-8  # Epsilon for numerical stability\n",
        "\n",
        "\n",
        "final_w, final_b, wlist, blist, losslist, accuracylist = adam_gradient_descent(X, Y, w, b, epochs, alpha, beta1, beta2, eps)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(wlist)\n",
        "plt.title('Change in Weight')"
      ]
    }
  ]
}